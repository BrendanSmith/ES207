---
title: 'Assignment 6: Multivariate Data Mining'
author: "Brendan Smith"
date: "April 25, 2016"
output: pdf_document
---

**Objective Statement:** The objective of this homework assignment is to become familiar with the process of data mining. Data mining is a process of discerning patterns of entire datasets, without always necessarily knowing the origin of the data. This can be both good and bad, as it can eliminate some bias from the data analysis process. We are expected to deliver a hydrogeomorphic model (HGM) that is validated using cluster analysis.

**Methods:** We will build an HGM given a set of new data, and a DEM from the previous homework assignment. 

**Data:** We are given a data set taken from several meadows across the Sierra Nevada. They are stored in a database file (dbf), and need additional grooming before they can be utilized for analysis.

**Code:**
```{r setup, include=FALSE}
# Include libraries
library(foreign)
library(raster)
library(rgdal)
library(stats)

mdws<-read.dbf("Sierra_Nevada_MultiSource_Meadow_Polygons_Compilation_v1.dbf")
View(mdws)

mdwhgm<-mdws[!is.na(mdws[,"HGM_TYPE"]),]
```

The function `is.na()` is the 'Not Available' function, which checks the dataframe to see where those elements are missing. Prepending the `!` operator to `is.na()` causes the function to return the element indeces that do in fact contain the value or characters placed in the 'Not Available' function. Thus, in this instance, we search for `HGM_TYPE` and create a new dataframe that only contains the rows that contain a a recorded hydrogeomorphic type.

We now add new columns to the newly created dataframe that are slightly more meaninful and easily discerned:
```{r}
# Suggested additions
mdwhgm$area.sqkm = mdwhgm[,"Shape_Area"]/1000000 # m^2 to km^2
mdwhgm$catch.sqkm = mdwhgm[,"CATCHMENT_"]/1000000# m^2 to km^2
mdwhgm$elev_m = mdwhgm[,"ELEV_MEAN"]
mdwhgm$elev_r = mdwhgm[,"ELEV_RANGE"]
mdwhgm$lat_dd = mdwhgm[,"LAT_DD"]
mdwhgm$lon_dd = mdwhgm[,"LONG_DD"]
mdwhgm$slope.pct = mdwhgm[,"FLOW_SLOPE"]
mdwhgm$edge.comp = mdwhgm[,"EDGE_COMPL"]
mdwhgm$clay = mdwhgm[,"ClayTot_r"]
mdwhgm$soil.kf = mdwhgm[,"Kf"]

```
 
**Results:**We begin by performing a quick EDA and then attempt to keep track of the relevant variables for data analysis.

##Step 1 - EDA and Scatter-plot matrices
```{r}
# EDA
summary(mdwhgm)
boxplot(mdwhgm)

#Optional method for keeping track of the relevant variables
rel_cols = c("area.sqkm", "catch.sqkm", "elev_m", "elev_r", "lat_dd", "lon_dd", "slope.pct", "edge.comp", "clay", "soil.kf")

rmdwhgm <-mdwhgm[,rel_cols]

```
We plot a scatter matrix to determine whether we have chosen enough relevant variables, and additionally analyze which variable pairs have the most variability and are highly correlated.
```{r, fig.width=9, fig.height=9}
plot(rmdwhgm)
```
Based on the plot, we determined that the most variability is found in the following pairs:
-Elev Mean & Lat
-Soil
-Lat
-Lon
-Elev mean
And the most correlated pairs are:
-Soil & Clay
-Lat & Clay
-Lat & Elev Mean
-elev mean & Lon
-Edge & Elev Range
-Elev Mean & Lon
-Elev Mean & Clay

##Step 2 - Clustering and Clustering Output
We now cluster the data using the `hclust()` function for hierarchical clustering. To use this method of clustering, we first find the euclidean distance. 
```{r,visibility=FALSE}
# Heirarchical Clustering
#dist using euclidean
plot.new()
rmdwhgm.dist<- dist(x = rmdwhgm[,rel_cols],method = "euclidean") #hclust using ward.D
rmdwhgm.hc<- hclust(rmdwhgm.dist,method="ward.D")
rect.hclust(rmdwhgm.hc,k=6)
```

We follow this up with k-means clustering via the `kmeans()` function. 

```{r}
# k-means Clustering
rmdwhgm$hc6 <- cutree(rmdwhgm.hc, k=6) #store group # in hc6
rmdwhgm.km6 <- kmeans(rmdwhgm[,rel_cols],centers = 6)
rmdwhgm$km6 <- rmdwhgm.km6$cluster #store group # in km6
table(rmdwhgm$hc6, rmdwhgm$km6)
```


```{r, fig.width=9, fig.height=9}
# Load the DEM
gdal_grid = readGDAL("DEM.tif")
dem = raster(gdal_grid) #use data as a projected raster
plot(dem,col=gray.colors(10, start=0.9, end=0.3))

# Create a vector to aid in plotting text for ProjLoc$ProjCode
xtext = rmdwhgm$lon_dd
ytext = rmdwhgm$lat_dd

# Plot the ProjLoc over the DEM
points(rmdwhgm$lon_dd,rmdwhgm$lat_dd,pch=19,col=rmdwhgm$hc6)
legend("topright",legend=levels(as.factor(rmdwhgm$hc6)),col=1:length(rmdwhgm$hc6),pch=19)
```


```{r, fig.width=9, fig.height=9}
plot(dem,col=gray.colors(10, start=0.9, end=0.3))

# Create a vector to aid in plotting text for ProjLoc$ProjCode
xtext = rmdwhgm$lon_dd
ytext = rmdwhgm$lat_dd

# Plot the ProjLoc over the DEM
points(rmdwhgm$lon_dd,rmdwhgm$lat_dd,pch=19,col=rmdwhgm$km6)
legend("topright",legend=levels(as.factor(rmdwhgm$km6)),col=1:length(rmdwhgm$km6),pch=19)
```


##Step 3 - Principal Components Analysis (PCA)

```{r}
rmdwhgm.pca <- prcomp(x = rmdwhgm[,rel_cols], scale=TRUE, retx = TRUE, center = TRUE, scores=TRUE)

summary(rmdwhgm.pca)
screeplot(rmdwhgm.pca)
plot(rmdwhgm.pca, type="lines", main="PCA of Relevant Variables")
title(xlab="Components")
```

```{r}
print(rmdwhgm.pca$rotation)
# Which parameters are driving the variability in the meadow dataset 
# (i.e., highest value)? Are these positive or negative loadings?
```

Driving the Loading:
PC1: positive loading, driven by latitude, however, much negative loading is seen
PC2: positive loading, edge complexity
PC3: positive loading, slope percent
PC4: positive loading, slope percent, negative loading, catch square km
PC5: positive loading, catch square km

```{r, fig.width=9, fig.height=9}
# Cycle throught the most important axes using a for loop, going from
# components 1 to 6
j<-1:5
for(i in j) {biplot(rmdwhgm.pca, choices=i:(i+1), cex=0.5, xlim=c(-0.1,0.2), ylim=c(- 0.1,0.2))}
```

```{r}
pairs(rmdwhgm.pca$x[,1:3],col=rmdwhgm$km6)#colored by Kmeans group
pairs(rmdwhgm.pca$x[,1:3],col=rmdwhgm$hc6)#colored by HClust group
```

##Step 4 - Contingency Analysis of Hydrogeomorphic Type

```{r}
chisq.test(table(rmdwhgm$hc6, rmdwhgm$km6))
# Does there appear to be relationship based on counts? 
# Is there a statistical relationship?
```

##Step 5 - Summarize the Data by National Forest


**Discussion:** Overall, a very in depth homework assignment. We were able to assemble an HGM from a dataset and verify it with Principal Component Analysis.

**Limitations:** There really were no limitations during this assignment.